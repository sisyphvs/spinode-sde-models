---
# About this project
## Physicsâ€“Informed Neural SDEs: Implementation of GBM and Heston

## Based on the Stochastic Physics-Informed Neural ODEs methodology (Oâ€™Leary, Paulson & Mesbah, 2023)

### 1. Introduction

This repository implements a simplified and pedagogical version of the Stochastic Physics-Informed Neural Ordinary Differential Equations (SPIN-ODEs) framework originally proposed by Oâ€™Leary, Paulson, and Mesbah (2023).

The original work introduces a general methodology for training neural models to learn stochastic dynamics while enforcing physical or structural constraints derived from stochastic differential equations (SDEs).

In our case, we extend the base example provided in the reference code â€”the Lotkaâ€“Volterra systemâ€” and implement two additional experiments based on widely used models in quantitative finance:

- Geometric Brownian Motion (GBM)

- Heston Stochastic Volatility Model

The goal is to demonstrate that the framework is modular, allowing the underlying dynamical system to be replaced without altering the core training architecture.

### 2. Motivation

The primary motivation behind this project is to explore whether the SPIN-ODE methodology can:

Reliably separate drift and diffusion within a neural training pipeline,

Learn parameters and trajectories even when the noise source is not directly observable,

Extend beyond the Lotkaâ€“Volterra example while preserving numerical stability, stochastic fidelity, and computational efficiency.

This repository serves as a pedagogical laboratory, where several elements of the original framework are simplified to facilitate study, visualization, and reproducibility.

### 3. Connection to the Original Work

Our implementation is built upon the reference application provided by Jared Oâ€™Leary, associated with the SPIN-ODE publication:

O'Leary, J., Paulson, J.A., & Mesbah, A. (2023).
Stochastic Physics-Informed Neural Ordinary Differential Equations.
University of California, Berkeley / The Ohio State University.

In particular:

We preserve the modular training structure,

We follow the same philosophy of splitting learning into train_g1 (drift) and train_g2 (diffusion),

We adapt the data generation pipeline to support GBM and Heston,

We simplify selected components to make the experiments easier to interpret and replicate.

### 4. General Framework Structure

#### 4.1 Driftâ€“Diffusion Decomposition

Following the methodology of Oâ€™Leary et al., the training pipeline is divided into two complementary phases:

- Phase G1 (train_g1)

Learning the deterministic component of the SDE (the drift) using observed trajectories.

- Phase G2 (train_g2)

Learning the stochastic component (the diffusion) using the residual increments between the learned drift and the true state transitions.

This decomposition is valid only when the SDE admits a representation of the form:


$$
\begin{aligned}
dX_t &= f(X_t,t)\,dt + g(X_t,t)\,dW_t
\end{aligned}
$$

and when an explicit Eulerâ€“Maruyama simulator is available so that the drift and diffusion components can be separated from synthetic data.

### 5. Implemented Models
5.1 Geometric Brownian Motion (GBM)


$$
\begin{aligned}
dS_t &= \mu S_t dt + \sigma S_t dW_t
\end{aligned}
$$


Relevant characteristics:

Drift and diffusion are fully separable.

Serves as a baseline for numerical stability and accuracy of the framework.

Enables direct comparison with the closed-form analytical solution.

### 5.2 Heston Model

$$
\begin{aligned}
dS_t &= \mu S_t\,dt + \sqrt{v_t}\,S_t\,dW_t^{(1)},\\
dv_t &= \kappa(\theta - v_t)\,dt + \xi\sqrt{v_t}\,dW_t^{(2)}
\end{aligned}
$$

â€‹
with correlation

$$
\begin{aligned}
\rho &= \mathrm{corr}(dW_t^{(1)}, dW_t^{(2)})
\end{aligned}
$$

Key features:

Two-dimensional stochastic system with structured diffusion.

Requires consistency between correlated noise sources.

Tests the frameworkâ€™s ability to learn coupled stochastic dynamics.

### 6. Code Architecture

The repository is structured as follows:
   ```bash
     SPINODE_extended/
  â”œâ”€â”€ src/
  â”‚   â”œâ”€â”€ dynamics.py          # CSA/LVE/SIR + BS + Heston
  â”‚   â”œâ”€â”€ train.py             # funciones genÃ©ricas de entrenamiento g1/g2
  â”‚   â”œâ”€â”€ reconstruct.py       # reconstruct_CSA + reconstruct_LVE/BS/Heston
  â”‚   â”œâ”€â”€ utils.py
  â”‚   â””â”€â”€ __init__.py          # opcional, si quieres tratarlo como paquete
  â”‚
  â”œâ”€â”€ experiments/
  â”‚   â”œâ”€â”€ LVE/
  â”‚   â”‚   â”œâ”€â”€ run_LVE.ipynb
  â”‚   â”‚   â”œâ”€â”€ generate_LVE_data.ipynb
  â”‚   â”‚   â”œâ”€â”€ LVE_data.ipynb
  â”‚   â”‚   â”œâ”€â”€ data/            # .npy, .npz, etc. (ignorados en git si son pesados)
  â”‚   â”‚   â””â”€â”€ results/         # figuras, rmse, etc.
  â”‚   â”œâ”€â”€ BS/
  â”‚   â”‚   â”œâ”€â”€ run_BS.ipynb
  â”‚   â”‚   â”œâ”€â”€ generate_BS_data.ipynb
  â”‚   â”‚   â”œâ”€â”€ BS_data.ipynb
  â”‚   â”‚   â”œâ”€â”€ data/
  â”‚   â”‚   â””â”€â”€ results/
  â”‚   â””â”€â”€ Heston/
  â”‚       â”œâ”€â”€ run_Heston.ipynb
  â”‚       â”œâ”€â”€ generate_Heston_data.ipynb
  â”‚       â”œâ”€â”€ Heston_data.ipynb
  â”‚       â”œâ”€â”€ data/
  â”‚       â””â”€â”€ results/
  â”‚
  â”œâ”€â”€ notebooks/               # (opcional) notebooks mÃ¡s generales, p.ej. run.ipynb
  â”‚   â””â”€â”€ run_all_models.ipynb
  â”‚
  â”œâ”€â”€ README.md
  â”œâ”€â”€ requirements.txt / env.yml
  â””â”€â”€ .gitignore
   ```

#### 6.1 train_g1

This module trains the drift by fitting:

$$
\begin{aligned}
\widehat{f}_{\theta}(x,t) &\approx \frac{X_{t+\Delta t}-X_t}{\Delta t}
\end{aligned}
$$

#### 6.2 train_g2

With the drift already learned, the module estimates the noise structure, recovering:

$$
\begin{aligned}
\widehat{g}_{\phi}(x,t) &\approx 
\frac{X_{t+\Delta t}-X_t-\widehat{f}_{\theta}(x,t)\,\Delta t}{\sqrt{\Delta t}}
\end{aligned}
$$

### 7. Expected Results

In GBM, the method recovers parameters close to the true 
ğœ‡
Î¼ and 
ğœ
Ïƒ.

In Heston, the method reproduces coherent trajectories and captures the volatility-driven noise structure.

The approach is numerically stable and extensible to other separable driftâ€“diffusion models.

### 8. Known Limitations

This methodology is not directly applicable when:

Drift and diffusion cannot be cleanly separated.

The diffusion term is not driven by Gaussian noise.

The dynamics involve jumps or non-Eulerâ€“Maruyama noise (e.g., LÃ©vy processes).

The system exhibits degenerate diffusion that cannot be extracted from finite-difference increments.

### 9. Bibliographic Reference

This project draws upon and integrates ideas from the following works:

- **O'Leary, J., Paulson, J.A., & Mesbah, A. (2023).**  
  *Stochastic Physics-Informed Neural Ordinary Differential Equations.*  
  University of California, Berkeley / The Ohio State University.

- **El Janati Elidrissi, Y., & Efstathiadis, G. (2023).**  
  *PINN-Based SDE Solver.*  
  Harvard T.H. Chan School of Public Health.

- **OlguÃ­n, D. (2024).**  
  *The Math Behind the Magic: Neural Networks, Theory and Practice.*  
  Encuentro Nacional de IngenierÃ­a MatemÃ¡tica 2024,  
  with J. Fontbona, J. Maass, and C. MuÃ±oz.

